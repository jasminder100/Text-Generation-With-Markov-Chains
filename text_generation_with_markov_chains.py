# -*- coding: utf-8 -*-
"""Text Generation With Markov Chains.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZeYp0lWfBOpSgrijPulu8zXMdr8KOki
"""

!pip install pandas
!pip install markovify
!pip install spacy

import pandas as pd
import markovify
import spacy

df = pd.read_csv("hamlets.csv")

df.head()

print(df.columns)

print(df["language"].value_counts())

english_text = df[df["language"] == "English"]["text"].iloc[0]

print(english_text[:500])

word_count = len(english_text.split())
print("Total words:", word_count)

import re

def basic_clean(text):
    text = re.sub(r'\s+', ' ', text)   # replace multiple spaces/newlines with single space
    return text.strip()

clean_text = basic_clean(english_text)
print(clean_text[:500])

def remove_headings(text):
    text = re.sub(r'Actus\s+\w+|ACT\s+\w+', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Scoena\s+\w+|SCENE\s+\w+', '', text, flags=re.IGNORECASE)
    return text

clean_text = remove_headings(clean_text)
print(clean_text[:500])

def remove_stage_directions(text):
    text = re.sub(r'Enter\s+[A-Za-z\s]+', '', text)
    text = re.sub(r'Exit\s+[A-Za-z\s]+', '', text)
    return text

clean_text = remove_stage_directions(clean_text)

def remove_speaker_labels(text):
    text = re.sub(r'\b[A-Z][a-z]+\:', '', text)
    return text

clean_text = remove_speaker_labels(clean_text)

clean_text = re.sub(r'\s+', ' ', clean_text).strip()
print(clean_text[:500])

!python -m spacy download en_core_web_sm

nlp = spacy.load("en_core_web_sm")
doc = nlp(clean_text)

sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 1]

for i in range(5):
    print(sentences[i])

markov_text = " ".join(sentences)

print(markov_text[:500])

print("Total sentences:", len(sentences))

print("Total words:", len(markov_text.split()))

text_model = markovify.Text(markov_text, state_size=2)

for i in range(5):
    print(text_model.make_sentence())

for i in range(5):
    print(text_model.make_short_sentence(max_chars=100))

model_1 = markovify.Text(markov_text, state_size=1)
print(model_1.make_sentence())

model_3 = markovify.Text(markov_text, state_size=3)
print(model_3.make_sentence())

def generate_sentence(model):
    sentence = None
    while sentence is None:
        sentence = model.make_sentence()
    return sentence

for i in range(5):
    print(generate_sentence(text_model))

def generate_clean_sentence(model, max_chars=140, tries=20):
    for _ in range(tries):
        sentence = model.make_short_sentence(
            max_chars=max_chars,
            tries=tries
        )
        if sentence:
            return sentence
    return "No valid sentence generated."

for _ in range(5):
    print(generate_clean_sentence(text_model))

class POSifiedText(markovify.Text):
    def word_split(self, sentence):
        return [
            "::".join((token.text, token.pos_))
            for token in nlp(sentence)
        ]

    def word_join(self, words):
        return " ".join(word.split("::")[0] for word in words)

pos_model = POSifiedText(markov_text, state_size=2)

for _ in range(5):
    print(generate_clean_sentence(pos_model))

def preprocess_text(raw_text):
    text = re.sub(r'\s+', ' ', raw_text)
    text = re.sub(r'Actus\s+\w+|Scoena\s+\w+', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Enter\s+[A-Za-z\s]+', '', text)
    return text.strip()

def sentence_tokenize(text, nlp):
    doc = nlp(text)
    return " ".join(sent.text for sent in doc.sents if len(sent.text) > 1)

def build_markov_model(text, state_size=2, pos=False):
    if pos:
        return POSifiedText(text, state_size=state_size)
    return markovify.Text(text, state_size=state_size)

cleaned = preprocess_text(english_text)
markov_ready = sentence_tokenize(cleaned, nlp)

model = build_markov_model(markov_ready, state_size=2, pos=True)

for _ in range(5):
    print(generate_clean_sentence(model))